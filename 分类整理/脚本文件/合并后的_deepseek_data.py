#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆå¹¶åçš„DeepSeekæ•°æ®å¤„ç†è„šæœ¬
æ•´åˆäº†æ‰€æœ‰DeepSeekæ•°æ®å¤„ç†ç›¸å…³åŠŸèƒ½

åŠŸèƒ½åŒ…æ‹¬ï¼š
1. DeepSeekæ•°æ®æ•´åˆä¸è®¿é—®
2. å¯¹è¯æ•°æ®å¯¼å‡º
3. Cozeæ’ä»¶å¯¼å‡ºå’Œç®¡ç†
4. æ’ä»¶åˆå¹¶åŠŸèƒ½
"""

import json
import os
import sys
import argparse
import logging
import zipfile
import tempfile
import shutil
import hashlib
from datetime import datetime
from typing import Dict, Any, List, Optional

# è®¾ç½®æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('merged_deepseek_data.log', encoding='utf-8')
    ]
)
logger = logging.getLogger('MergedDeepSeekData')

class DeepSeekDataIntegrator:
    """
    DeepSeekæ•°æ®æ•´åˆå™¨ - ç”¨äºåŠ è½½ã€å¤„ç†å’Œè®¿é—®DeepSeekå¯¹è¯æ•°æ®
    é‡‡ç”¨æŒ‰éœ€åŠ è½½å’Œè¿­ä»£è®¿é—®æ¨¡å¼ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨
    """
    
    def __init__(self, data_dir="./deepseek_data"):
        """
        åˆå§‹åŒ–æ•°æ®æ•´åˆå™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
        """
        self.data_dir = data_dir
        self.conversations_file = os.path.join(data_dir, "conversations.json")
        self.user_file = os.path.join(data_dir, "user.json")
        
        # åŠ è½½ç”¨æˆ·ä¿¡æ¯ï¼ˆè¾ƒå°æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½ï¼‰
        self.user_info = self._load_user_info()
        
        # é¢„è®¡ç®—å¯¹è¯æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿè®¿é—®ï¼‰
        self._conversation_count = None
    
    def _load_user_info(self):
        """
        åŠ è½½ç”¨æˆ·ä¿¡æ¯
        
        Returns:
            dict: ç”¨æˆ·ä¿¡æ¯å­—å…¸
        """
        try:
            with open(self.user_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½ç”¨æˆ·ä¿¡æ¯æ–‡ä»¶ {self.user_file}: {e}")
            return {}
    
    @property
    def conversation_count(self):
        """
        è·å–å¯¹è¯æ€»æ•°
        
        Returns:
            int: å¯¹è¯æ€»æ•°
        """
        if self._conversation_count is None:
            self._conversation_count = self._count_conversations()
        return self._conversation_count
    
    def _count_conversations(self):
        """
        è®¡ç®—å¯¹è¯æ€»æ•°
        
        Returns:
            int: å¯¹è¯æ€»æ•°
        """
        try:
            with open(self.conversations_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return len(data)
        except Exception as e:
            logger.warning(f"æ— æ³•è®¡ç®—å¯¹è¯æ•°é‡ {self.conversations_file}: {e}")
            return 0
    
    def get_conversations(self, start=0, end=None):
        """
        æŒ‰èŒƒå›´è·å–å¯¹è¯
        
        Args:
            start: èµ·å§‹ç´¢å¼•
            end: ç»“æŸç´¢å¼•ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            
        Yields:
            dict: å¯¹è¯å¯¹è±¡
        """
        try:
            with open(self.conversations_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if end is None:
                    end = len(data)
                
                for i in range(start, min(end, len(data))):
                    yield data[i]
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½å¯¹è¯æ•°æ® {self.conversations_file}: {e}")
            return
    
    def get_conversation_by_id(self, conversation_id):
        """
        é€šè¿‡IDè·å–å¯¹è¯
        
        Args:
            conversation_id: å¯¹è¯ID
            
        Returns:
            dict or None: å¯¹è¯å¯¹è±¡æˆ–None
        """
        try:
            with open(self.conversations_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for conv in data:
                    if conv.get('id') == conversation_id:
                        return conv
                return None
        except Exception as e:
            logger.warning(f"æ— æ³•æŸ¥æ‰¾å¯¹è¯ {conversation_id}: {e}")
            return None
    
    def get_conversations_by_keyword(self, keyword, case_insensitive=True):
        """
        é€šè¿‡å…³é”®è¯æœç´¢å¯¹è¯
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            case_insensitive: æ˜¯å¦å¿½ç•¥å¤§å°å†™
            
        Yields:
            dict: åŒ¹é…çš„å¯¹è¯å¯¹è±¡
        """
        try:
            with open(self.conversations_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                for conv in data:
                    # æœç´¢æ ‡é¢˜
                    title = conv.get('title', '')
                    if case_insensitive:
                        title_lower = title.lower()
                        keyword_lower = keyword.lower()
                        if keyword_lower in title_lower:
                            yield conv
                            continue
                    else:
                        if keyword in title:
                            yield conv
                            continue
                    
                    # æœç´¢æ¶ˆæ¯å†…å®¹
                    mapping = conv.get('mapping', {})
                    for node_id, node in mapping.items():
                        if node_id == 'root':
                            continue
                        
                        message = node.get('message', {})
                        fragments = message.get('fragments', [])
                        for fragment in fragments:
                            content = fragment.get('content', '')
                            if case_insensitive:
                                content_lower = content.lower()
                                keyword_lower = keyword.lower()
                                if keyword_lower in content_lower:
                                    yield conv
                                    break
                            else:
                                if keyword in content:
                                    yield conv
                                    break
                        else:
                            continue
                        break
        except Exception as e:
            logger.warning(f"æ— æ³•æœç´¢å¯¹è¯ {keyword}: {e}")
            return
    
    def get_user_info(self):
        """
        è·å–ç”¨æˆ·ä¿¡æ¯
        
        Returns:
            dict: ç”¨æˆ·ä¿¡æ¯
        """
        return self.user_info
    
    def export_to_single_file(self, output_path):
        """
        å°†æ‰€æœ‰æ•°æ®å¯¼å‡ºä¸ºå•ä¸€æ–‡ä»¶
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        try:
            with open(self.conversations_file, 'r', encoding='utf-8') as f:
                conversations = json.load(f)
            
            # åˆ›å»ºå®Œæ•´æ•°æ®ç»“æ„
            complete_data = {
                "metadata": {
                    "version": "1.0.0",
                    "created_at": datetime.now().isoformat(),
                    "source": "DeepSeek Data Export",
                    "compatibility": {
                        "trae_ai_ide": True,
                        "trae_cn": True
                    },
                    "stats": {
                        "conversation_count": len(conversations),
                        "user_count": 1 if self.user_info else 0
                    }
                },
                "user_info": self.user_info,
                "conversations": conversations
            }
            
            # å†™å…¥æ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(complete_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æˆåŠŸå¯¼å‡ºåˆ°æ–‡ä»¶: {output_path}")
            return True
        except Exception as e:
            logger.error(f"æ— æ³•å¯¼å‡ºæ•°æ®åˆ°æ–‡ä»¶ {output_path}: {e}")
            return False
    
    def validate_data(self):
        """
        éªŒè¯æ•°æ®å®Œæ•´æ€§
        
        Returns:
            dict: éªŒè¯ç»“æœ
        """
        results = {
            "user_info_valid": bool(self.user_info),
            "conversations_exist": os.path.exists(self.conversations_file),
            "conversation_count": self.conversation_count,
            "errors": []
        }
        
        # éªŒè¯å¯¹è¯æ•°æ®ç»“æ„
        try:
            sample_conv = next(self.get_conversations(start=0, end=1), None)
            if sample_conv:
                required_fields = ['id', 'title', 'inserted_at', 'updated_at', 'mapping']
                for field in required_fields:
                    if field not in sample_conv:
                        results["errors"].append(f"å¯¹è¯ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")
        except Exception as e:
            results["errors"].append(f"éªŒè¯å¯¹è¯ç»“æ„å¤±è´¥: {e}")
        
        return results

class DeepSeekDataExporter:
    """
    DeepSeekæ•°æ®å¯¼å‡ºå™¨ - ç”¨äºå¯¼å‡ºå¯¹è¯æ•°æ®åˆ°ä¸åŒæ ¼å¼
    """
    
    def __init__(self, integrator):
        """
        åˆå§‹åŒ–æ•°æ®å¯¼å‡ºå™¨
        
        Args:
            integrator: DeepSeekDataIntegratorå®ä¾‹
        """
        self.integrator = integrator
    
    def export_conversations_to_txt(self, output_txt):
        """
        å°†å¯¹è¯å¯¼å‡ºä¸ºtxtæ ¼å¼
        
        Args:
            output_txt: è¾“å‡ºtxtæ–‡ä»¶è·¯å¾„
        """
        conversations = []
        
        # å…ˆå°è¯•ç›´æ¥è¯»å–åŸå§‹conversations.jsonæ–‡ä»¶
        original_conversations_file = os.path.join(self.integrator.data_dir, "conversations.json")
        if os.path.exists(original_conversations_file):
            try:
                with open(original_conversations_file, 'r', encoding='utf-8') as f:
                    conversations = json.load(f)
                logger.info(f"å·²ä»åŸå§‹æ–‡ä»¶è¯»å– {len(conversations)} æ¡å¯¹è¯")
            except Exception as e:
                logger.error(f"æ— æ³•è¯»å–åŸå§‹JSONæ–‡ä»¶ {original_conversations_file}: {e}")
                return False
        else:
            # å°è¯•ä»æ•´åˆå™¨è·å–å¯¹è¯
            conversations = list(self.integrator.get_conversations())
            logger.info(f"å·²ä»æ•´åˆå™¨è¯»å– {len(conversations)} æ¡å¯¹è¯")
        
        if not conversations:
            logger.error("æœªæ‰¾åˆ°ä»»ä½•å¯¹è¯æ•°æ®")
            return False
        
        # å‡†å¤‡è¾“å‡ºå†…å®¹
        output_lines = []
        output_lines.append("=" * 80)
        output_lines.append("DeepSeek å®Œæ•´å¯¹è¯å†…å®¹")
        output_lines.append("=" * 80)
        output_lines.append(f"å¯¼å‡ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        output_lines.append(f"å¯¹è¯æ€»æ•°: {len(conversations)}")
        output_lines.append("=" * 80)
        output_lines.append("")
        
        # éå†æ‰€æœ‰å¯¹è¯
        for conv_idx, conversation in enumerate(conversations, 1):
            # å¯¹è¯æ ‡é¢˜
            title = conversation.get('title', f"å¯¹è¯ {conv_idx}")
            created_at = conversation.get('inserted_at', '')
            
            output_lines.append(f"{conv_idx}. {title}")
            output_lines.append(f"åˆ›å»ºæ—¶é—´: {created_at}")
            output_lines.append("-" * 60)
            output_lines.append("")
            
            # éå†å¯¹è¯èŠ‚ç‚¹
            mapping = conversation.get('mapping', {})
            nodes = list(mapping.values())
            
            # æŒ‰é¡ºåºå¤„ç†èŠ‚ç‚¹
            for node in nodes:
                if node.get('id') == 'root':
                    continue
                
                message = node.get('message', {})
                fragments = message.get('fragments', [])
                
                for fragment in fragments:
                    frag_type = fragment.get('type', '')
                    content = fragment.get('content', '').strip()
                    
                    if not content:
                        continue
                    
                    # æ ¹æ®ç‰‡æ®µç±»å‹æ·»åŠ å‰ç¼€
                    if frag_type == 'REQUEST':
                        output_lines.append("ğŸ™‹ ç”¨æˆ·æé—®:")
                        output_lines.append(content)
                        output_lines.append("")
                    elif frag_type == 'RESPONSE':
                        output_lines.append("ğŸ¤– AIå›ç­”:")
                        output_lines.append(content)
                        output_lines.append("")
                    elif frag_type == 'THINK':
                        # æ€è€ƒè¿‡ç¨‹å¯ä»¥é€‰æ‹©æ€§åŒ…å«
                        pass
            
            output_lines.append("=" * 60)
            output_lines.append("")
        
        # å†™å…¥txtæ–‡ä»¶
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = os.path.dirname(output_txt)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir)
            
            # å°è¯•å†™å…¥æ–‡ä»¶
            with open(output_txt, 'w', encoding='utf-8') as f:
                f.write('\n'.join(output_lines))
            logger.info(f"æˆåŠŸå¯¼å‡ºå®Œæ•´å¯¹è¯åˆ°æ–‡ä»¶: {output_txt}")
            logger.info(f"æ–‡ä»¶å¤§å°: {os.path.getsize(output_txt)} å­—èŠ‚")
            return True
        except Exception as e:
            logger.error(f"æ— æ³•å†™å…¥txtæ–‡ä»¶ {output_txt}: {e}")
            
            # å°è¯•å†™å…¥åˆ°å¦ä¸€ä¸ªè·¯å¾„
            backup_output = "deepseek_conversations_backup.txt"
            try:
                with open(backup_output, 'w', encoding='utf-8') as f:
                    f.write('\n'.join(output_lines))
                logger.info(f"å·²å¤‡ä»½åˆ°: {backup_output}")
                return True
            except Exception as backup_e:
                logger.error(f"å¤‡ä»½ä¹Ÿå¤±è´¥äº†: {backup_e}")
                return False

class CozePluginManager:
    """
    Cozeæ’ä»¶ç®¡ç†å™¨ - ç”¨äºå¯¼å‡ºã€æ•´ç†å’Œåˆå¹¶Cozeæ’ä»¶
    """
    
    # ä»ç”¨æˆ·æä¾›çš„æˆªå›¾ä¸­æå–çš„æ’ä»¶æ•°æ®
    PLUGINS_DATA = [
        {
            "name": "Coze AI æ™ºèƒ½å·¥ä½œæµè‡ªåŠ¨",
            "description": "é›†æˆæ‰€æœ‰åŠŸèƒ½çš„AIæ™ºèƒ½å·¥ä½œæµè‡ªåŠ¨åŒ–å¤„ç†ç³»ç»Ÿ - å…¨è‡ªåŠ¨ä¸€é”®æ“ä½œ",
            "type": "æ’ä»¶",
            "edit_time": "2025-11-29 12:59:53",
            "plugin_id": "coze_ai_workflow_auto"
        },
        {
            "name": "efesgrhty",
            "description": "ygklh",
            "type": "æ’ä»¶",
            "edit_time": "2025-11-27 21:31:19",
            "plugin_id": "efesgrhty"
        },
        {
            "name": "Cozeå…¨åœºæ™¯æ™ºèƒ½è‡ªåŠ¨åŒ–56789",
            "description": "# Cozeå…¨åœºæ™¯æ™ºèƒ½è‡ªåŠ¨åŒ–è¶…çº§ä¸­æ¢ - å®Œæ•´ä¿®å¤ç‰ˆ## ğŸ“‹ é¡¹ç›®æ¦‚è¿°**é¡¹ç›®åç§°**: Cozeå…¨åœºæ™¯æ™ºèƒ½è‡ªåŠ¨åŒ–è¶…çº§ä¸­æ¢ **ç‰ˆæœ¬**: v10.1.0-Unified **æ ¸å¿ƒåŠŸèƒ½**: ç«¯åˆ°ç«¯è‡ªåŠ¨åŒ–",
            "type": "æ’ä»¶",
            "edit_time": "2025-11-27 20:44:55",
            "plugin_id": "coze_all_scene_auto_56789"
        }
    ]
    
    def __init__(self, export_dir=None):
        """
        åˆå§‹åŒ–æ’ä»¶ç®¡ç†å™¨
        
        Args:
            export_dir: å¯¼å‡ºç›®å½•è·¯å¾„
        """
        self.export_dir = export_dir or "c:\\Users\\Administrator\\Desktop\\erthhgfj\\å¯¼å‡ºçš„æ’ä»¶"
        self.coze_plugins_dir = os.path.join(self.export_dir, "coze_plugins")
        self.mcp_tools_dir = os.path.join(self.export_dir, "mcp_tools")
        
        # åˆ›å»ºå¯¼å‡ºç›®å½•
        os.makedirs(self.export_dir, exist_ok=True)
        os.makedirs(self.coze_plugins_dir, exist_ok=True)
        if not os.path.exists(self.mcp_tools_dir):
            os.makedirs(self.mcp_tools_dir, exist_ok=True)
    
    def calculate_file_hash(self, file_path):
        """
        è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            str: æ–‡ä»¶å“ˆå¸Œå€¼
        """
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    
    def create_plugin_files(self, plugin_info, plugin_dir):
        """
        ä¸ºå•ä¸ªæ’ä»¶åˆ›å»ºæ–‡ä»¶
        
        Args:
            plugin_info: æ’ä»¶ä¿¡æ¯
            plugin_dir: æ’ä»¶ç›®å½•
        """
        plugin_id = plugin_info["plugin_id"]
        plugin_name = plugin_info["name"]
        plugin_desc = plugin_info["description"]
        
        # åˆ›å»ºæ’ä»¶ç›®å½•
        os.makedirs(plugin_dir, exist_ok=True)
        
        # ä¿å­˜æ’ä»¶å…ƒæ•°æ®
        metadata = {
            "plugin_id": plugin_id,
            "name": plugin_name,
            "description": plugin_desc,
            "type": plugin_info["type"],
            "edit_time": plugin_info["edit_time"],
            "created_at": datetime.now().isoformat(),
            "version": "1.0.0"
        }
        
        metadata_path = os.path.join(plugin_dir, 'metadata.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # åˆ›å»ºæ’ä»¶ä»£ç æ–‡ä»¶
        plugin_code = f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
{plugin_name}

{plugin_desc}

æ’ä»¶ ID: {plugin_id}
ç‰ˆæœ¬: 1.0.0
ä½œè€…: Coze å¹³å°
åˆ›å»ºæ—¶é—´: {metadata['created_at']}
ç¼–è¾‘æ—¶é—´: {plugin_info['edit_time']}
"""

class {plugin_name.replace(' ', '').replace('-', '').replace('_', '')}:
    def __init__(self):
        self.plugin_id = "{plugin_id}"
        self.name = "{plugin_name}"
        
    def run(self, **kwargs):
        """æ’ä»¶æ‰§è¡Œå…¥å£"""
        return {{
            "success": True,
            "result": f"æ‰§è¡Œ {self.name} æ’ä»¶æˆåŠŸ",
            "plugin_id": self.plugin_id,
            "timestamp": datetime.now().isoformat()
        }}

# å¯¼å‡ºæ’ä»¶
if __name__ == "__main__":
    plugin = {plugin_name.replace(' ', '').replace('-', '').replace('_', '')}()
    result = plugin.run()
    print(result)
'''
        
        main_path = os.path.join(plugin_dir, 'main.py')
        with open(main_path, 'w', encoding='utf-8') as f:
            f.write(plugin_code)
        
        # åˆ›å»ºREADME.md
        readme_content = f"""# {plugin_name}

## æè¿°
{plugin_desc}

## åŸºæœ¬ä¿¡æ¯
- **æ’ä»¶ ID**: {plugin_id}
- **ç‰ˆæœ¬**: 1.0.0
- **ä½œè€…**: Coze å¹³å°
- **åˆ›å»ºæ—¶é—´**: {metadata['created_at']}
- **ç¼–è¾‘æ—¶é—´**: {plugin_info['edit_time']}
- **ç±»å‹**: {plugin_info['type']}

## ä½¿ç”¨è¯´æ˜
1. å®‰è£…ä¾èµ–
2. è¿è¡Œ `python main.py`
3. æˆ–å¯¼å…¥ä½¿ç”¨: `from main import {plugin_name.replace(' ', '').replace('-', '').replace('_', '')}`

## ç¤ºä¾‹
```python
from main import {plugin_name.replace(' ', '').replace('-', '').replace('_', '')}

plugin = {plugin_name.replace(' ', '').replace('-', '').replace('_', '')}()
result = plugin.run()
print(result)
```
"""
        
        readme_path = os.path.join(plugin_dir, 'README.md')
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        logger.info(f"æ’ä»¶ {plugin_name} å·²åˆ›å»ºåˆ° {plugin_dir}")
    
    def export_all_plugins(self):
        """
        å¯¼å‡ºæ‰€æœ‰æ’ä»¶
        """
        logger.info("å¼€å§‹å¯¼å‡ºæ‰€æœ‰æ’ä»¶...")
        
        # åˆ›å»ºæ’ä»¶ç´¢å¼•
        index_data = {
            "total_plugins": len(self.PLUGINS_DATA),
            "generated_at": datetime.now().isoformat(),
            "plugins": [],
            "categories": [],
            "tags": []
        }
        
        # ä¸ºæ¯ä¸ªæ’ä»¶åˆ›å»ºæ–‡ä»¶
        for plugin_info in self.PLUGINS_DATA:
            plugin_id = plugin_info["plugin_id"]
            plugin_dir = os.path.join(self.coze_plugins_dir, plugin_id)
            
            # åˆ›å»ºæ’ä»¶æ–‡ä»¶
            self.create_plugin_files(plugin_info, plugin_dir)
            
            # æ›´æ–°ç´¢å¼•æ•°æ®
            index_data["plugins"].append({
                "plugin_id": plugin_id,
                "name": plugin_info["name"],
                "description": plugin_info["description"],
                "author": "Coze å¹³å°",
                "version": "1.0.0",
                "category": "è‡ªåŠ¨åŒ–å·¥å…·",
                "tags": ["è‡ªåŠ¨åŒ–", "å·¥ä½œæµ", "AI"]
            })
        
        # ä¿å­˜ç´¢å¼•æ–‡ä»¶
        index_path = os.path.join(self.coze_plugins_dir, 'index.json')
        with open(index_path, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ‰€æœ‰æ’ä»¶å·²å¯¼å‡ºå®Œæˆï¼")
        logger.info(f"å¯¼å‡ºç›®å½•: {self.export_dir}")
        logger.info(f"æ’ä»¶æ•°é‡: {len(self.PLUGINS_DATA)}")
        logger.info(f"ç´¢å¼•æ–‡ä»¶: {index_path}")
    
    def merge_plugins(self):
        """
        åˆå¹¶æ‰€æœ‰æ’ä»¶æˆä¸€ä¸ªè¶…çº§æ’ä»¶
        """
        logger.info("å¼€å§‹åˆå¹¶æ’ä»¶...")
        
        # åŠ è½½æ‰€æœ‰æ’ä»¶æ•°æ®
        plugins_data = []
        
        # åŠ è½½Cozeæ’ä»¶
        if os.path.exists(self.coze_plugins_dir):
            # è¯»å–æ’ä»¶ç´¢å¼•
            index_path = os.path.join(self.coze_plugins_dir, 'index.json')
            if os.path.exists(index_path):
                with open(index_path, 'r', encoding='utf-8') as f:
                    index_data = json.load(f)
                
                # åŠ è½½æ¯ä¸ªæ’ä»¶çš„è¯¦ç»†ä¿¡æ¯
                for plugin_info in index_data.get('plugins', []):
                    plugin_id = plugin_info['plugin_id']
                    plugin_dir = os.path.join(self.coze_plugins_dir, plugin_id)
                    if os.path.exists(plugin_dir):
                        # è¯»å–æ’ä»¶å…ƒæ•°æ®
                        metadata_path = os.path.join(plugin_dir, 'metadata.json')
                        if os.path.exists(metadata_path):
                            with open(metadata_path, 'r', encoding='utf-8') as f:
                                metadata = json.load(f)
                            
                            # è¯»å–æ’ä»¶ä»£ç 
                            main_path = os.path.join(plugin_dir, 'main.py')
                            code = ""
                            if os.path.exists(main_path):
                                with open(main_path, 'r', encoding='utf-8') as f:
                                    code = f.read()
                            
                            # è¯»å–README
                            readme_path = os.path.join(plugin_dir, 'README.md')
                            readme = ""
                            if os.path.exists(readme_path):
                                with open(readme_path, 'r', encoding='utf-8') as f:
                                    readme = f.read()
                            
                            plugins_data.append({
                                'type': 'coze_plugin',
                                'id': plugin_id,
                                'metadata': metadata,
                                'code': code,
                                'readme': readme
                            })
        
        # åŠ è½½MCPå·¥å…·
        if os.path.exists(self.mcp_tools_dir):
            for filename in os.listdir(self.mcp_tools_dir):
                if filename.endswith('.json'):
                    mcp_file = os.path.join(self.mcp_tools_dir, filename)
                    with open(mcp_file, 'r', encoding='utf-8') as f:
                        mcp_data = json.load(f)
                    
                    plugins_data.append({
                        'type': 'mcp_tool',
                        'id': filename[:-5],  # å»é™¤.jsonåç¼€
                        'metadata': mcp_data.get('metadata', {}),
                        'config': mcp_data.get('config', {}),
                        'content': mcp_data.get('content', {})
                    })
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(plugins_data)} ä¸ªæ’ä»¶")
        
        # åˆ›å»ºè¶…çº§æ’ä»¶åŸºç¡€ç»“æ„
        super_plugin = {
            'metadata': {
                'name': 'è¶…çº§Cozeæ’ä»¶',
                'description': 'èåˆäº†æ‰€æœ‰æ’ä»¶åŠŸèƒ½çš„è¶…çº§æ’ä»¶',
                'version': '1.0.0',
                'created_at': datetime.now().isoformat(),
                'author': 'Coze æ’ä»¶åˆå¹¶å·¥å…·',
                'plugin_id': 'super_coze_plugin',
                'type': 'è¶…çº§æ’ä»¶',
                'total_plugins_merged': len(plugins_data),
                'coze_plugins_count': len([p for p in plugins_data if p['type'] == 'coze_plugin']),
                'mcp_tools_count': len([p for p in plugins_data if p['type'] == 'mcp_tool'])
            },
            'config': {
                'security_level': 'high',
                'allowed_commands': [],
                'max_concurrent_calls': 10,
                'timeout': 60,
                'security': {
                    'sandbox_enabled': True,
                    'javascript_restricted': True,
                    'cookie_isolation': True,
                    'origin_restriction': True,
                    'popup_blocking': True,
                    'ad_blocking': True
                }
            },
            'plugins': {},
            'categories': [],
            'tags': []
        }
        
        # åˆå¹¶æ‰€æœ‰æ’ä»¶
        for plugin in plugins_data:
            plugin_id = plugin['id']
            super_plugin['plugins'][plugin_id] = plugin
            
            # æå–åˆ†ç±»
            if 'category' in plugin.get('metadata', {}):
                category = plugin['metadata']['category']
                if category not in super_plugin['categories']:
                    super_plugin['categories'].append(category)
            
            # æå–æ ‡ç­¾
            if 'tags' in plugin.get('metadata', {}):
                for tag in plugin['metadata']['tags']:
                    if tag not in super_plugin['tags']:
                        super_plugin['tags'].append(tag)
            
            # åˆå¹¶å®‰å…¨é…ç½®
            if 'config' in plugin and 'allowed_commands' in plugin['config']:
                for cmd in plugin['config']['allowed_commands']:
                    if cmd not in super_plugin['config']['allowed_commands']:
                        super_plugin['config']['allowed_commands'].append(cmd)
        
        # ä¿å­˜è¶…çº§æ’ä»¶å…ƒæ•°æ®
        super_plugin_path = os.path.join(self.export_dir, 'super_coze_plugin.json')
        with open(super_plugin_path, 'w', encoding='utf-8') as f:
            json.dump(super_plugin, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æˆåŠŸåˆå¹¶ {len(plugins_data)} ä¸ªæ’ä»¶")
        logger.info(f"è¶…çº§æ’ä»¶å·²ä¿å­˜åˆ°: {super_plugin_path}")
        
        return super_plugin

def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description='åˆå¹¶åçš„DeepSeekæ•°æ®å¤„ç†è„šæœ¬')
    
    # æ•°æ®æ•´åˆå’Œè®¿é—®ç›¸å…³å‘½ä»¤
    parser.add_argument('--data-dir', default='./deepseek_data', help='æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--export-json', type=str, help='å¯¼å‡ºæ•°æ®ä¸ºJSONæ–‡ä»¶')
    parser.add_argument('--export-txt', type=str, help='å¯¼å‡ºå¯¹è¯ä¸ºTXTæ–‡ä»¶')
    parser.add_argument('--stats', action='store_true', help='æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯')
    parser.add_argument('--validate', action='store_true', help='éªŒè¯æ•°æ®å®Œæ•´æ€§')
    parser.add_argument('--search', type=str, help='æœç´¢å…³é”®è¯')
    
    # æ’ä»¶ç›¸å…³å‘½ä»¤
    parser.add_argument('--export-plugins', action='store_true', help='å¯¼å‡ºCozeæ’ä»¶')
    parser.add_argument('--merge-plugins', action='store_true', help='åˆå¹¶æ‰€æœ‰æ’ä»¶ä¸ºè¶…çº§æ’ä»¶')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ•°æ®æ•´åˆå™¨
    integrator = DeepSeekDataIntegrator(args.data_dir)
    
    # æ•°æ®å¯¼å‡ºå‘½ä»¤
    if args.export_json:
        logger.info(f"å¯¼å‡ºæ•°æ®åˆ°JSONæ–‡ä»¶: {args.export_json}")
        if integrator.export_to_single_file(args.export_json):
            logger.info("æ•°æ®å¯¼å‡ºæˆåŠŸ")
        else:
            logger.error("æ•°æ®å¯¼å‡ºå¤±è´¥")
    elif args.export_txt:
        logger.info(f"å¯¼å‡ºå¯¹è¯åˆ°TXTæ–‡ä»¶: {args.export_txt}")
        exporter = DeepSeekDataExporter(integrator)
        if exporter.export_conversations_to_txt(args.export_txt):
            logger.info("å¯¹è¯å¯¼å‡ºæˆåŠŸ")
        else:
            logger.error("å¯¹è¯å¯¼å‡ºå¤±è´¥")
    
    # æ•°æ®ç»Ÿè®¡å‘½ä»¤
    elif args.stats:
        logger.info("è·å–å¯¹è¯ç»Ÿè®¡ä¿¡æ¯...")
        print(f"æ€»å¯¹è¯æ•°: {integrator.conversation_count}")
        print(f"ç”¨æˆ·ä¿¡æ¯: {'æœ‰æ•ˆ' if integrator.user_info else 'æ— æ•ˆ'}")
        
    # æ•°æ®éªŒè¯å‘½ä»¤
    elif args.validate:
        logger.info("éªŒè¯æ•°æ®å®Œæ•´æ€§...")
        results = integrator.validate_data()
        print(json.dumps(results, ensure_ascii=False, indent=2))
    
    # æœç´¢å‘½ä»¤
    elif args.search:
        logger.info(f"æœç´¢å…³é”®è¯: {args.search}")
        count = 0
        for conv in integrator.get_conversations_by_keyword(args.search):
            print(f"å¯¹è¯ID: {conv.get('id')}")
            print(f"æ ‡é¢˜: {conv.get('title')}")
            print(f"åˆ›å»ºæ—¶é—´: {conv.get('inserted_at')}")
            print("-" * 50)
            count += 1
            if count >= 10:  # æœ€å¤šæ˜¾ç¤º10ä¸ªç»“æœ
                print(f"... è¿˜æœ‰æ›´å¤šç»“æœï¼Œå…±æ‰¾åˆ°{count}ä¸ªåŒ¹é…")
                break
        if count == 0:
            print("æœªæ‰¾åˆ°åŒ¹é…çš„å¯¹è¯")
    
    # æ’ä»¶ç›¸å…³å‘½ä»¤
    elif args.export_plugins:
        plugin_manager = CozePluginManager()
        plugin_manager.export_all_plugins()
    elif args.merge_plugins:
        plugin_manager = CozePluginManager()
        plugin_manager.merge_plugins()
    
    else:
        # é»˜è®¤æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
        parser.print_help()

if __name__ == "__main__":
    main()