#!/usr/bin/env python3
"""
AI_Automated_Workflow_Processo - å®Œæ•´çš„AIè‡ªåŠ¨åŒ–å·¥ä½œæµå¤„ç†å™¨

é›†æˆäº†å·¥ä½œæµä¿®å¤ã€é”™è¯¯æ£€æµ‹ã€äººå·¥æ™ºèƒ½ä»£ç åˆ†æã€å·¥ä½œæµç”Ÿæˆã€æ‰¹é‡å¤„ç†ã€ç”»å¸ƒè‡ªåŠ¨åŒ–å¡«å†™ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚
æ”¯æŒä»URLè·å–èµ„æºï¼Œè‡ªåŠ¨æ£€æµ‹å¹¶ä¿®å¤å·¥ä½œæµé”™è¯¯ï¼Œåˆ†æä»£ç è´¨é‡ï¼Œæ ¹æ®éœ€æ±‚ç”Ÿæˆæ–°å·¥ä½œæµï¼Œ
å®ç°å·¥ä½œæµçš„æ‰¹é‡ä¿®å¤å’Œåˆå¹¶ï¼Œä»¥åŠè‡ªåŠ¨åŒ–å¡«å†™Cozeç”»å¸ƒã€‚é€šè¿‡æ™ºèƒ½ç»“æœåˆå¹¶ï¼Œè¿”å›ç»Ÿä¸€çš„å¤„ç†ç»“æœå’Œç±»å‹ï¼Œ
æå‡å·¥ä½œæµå¼€å‘å’Œç»´æŠ¤æ•ˆç‡ã€‚
"""

import json
import requests
import re
import os
import time
from typing import Dict, List, Any, Optional, Tuple

class AIWorkflowProcessor:
    """AIè‡ªåŠ¨åŒ–å·¥ä½œæµå¤„ç†å™¨æ ¸å¿ƒç±»"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–å·¥ä½œæµå¤„ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«APIå¯†é’¥ã€æ¨¡å‹é…ç½®ç­‰
        """
        self.config = self._load_config(config)
        self.supported_workflow_formats = ["json", "yaml", "yml"]
        self.workflow_schema = self._load_workflow_schema()
        
    def _load_config(self, config: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """åŠ è½½é…ç½®
        
        Args:
            config: å¤–éƒ¨é…ç½®
            
        Returns:
            åˆå¹¶åçš„é…ç½®å­—å…¸
        """
        default_config = {
            "deepseek_api_key": "",
            "deepseek_api_url": "https://api.deepseek.com",
            "model": "deepseek-chat",
            "timeout": 60,
            "output_dir": "./workflow_output",
            "log_level": "info"
        }
        
        if config:
            default_config.update(config)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(default_config["output_dir"], exist_ok=True)
        
        return default_config
    
    def _load_workflow_schema(self) -> Dict[str, Any]:
        """åŠ è½½å·¥ä½œæµschema
        
        Returns:
            å·¥ä½œæµschemaå­—å…¸
        """
        # ç®€åŒ–çš„å·¥ä½œæµschemaï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥ä»æ–‡ä»¶æˆ–APIåŠ è½½
        return {
            "type": "object",
            "required": ["name", "description", "nodes", "edges"],
            "properties": {
                "name": {"type": "string"},
                "description": {"type": "string"},
                "version": {"type": "string"},
                "nodes": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "required": ["id", "type", "data"],
                        "properties": {
                            "id": {"type": "string"},
                            "type": {"type": "string"},
                            "data": {"type": "object"},
                            "position": {"type": "object"}
                        }
                    }
                },
                "edges": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "required": ["id", "source", "target"],
                        "properties": {
                            "id": {"type": "string"},
                            "source": {"type": "string"},
                            "target": {"type": "string"},
                            "sourceHandle": {"type": "string"},
                            "targetHandle": {"type": "string"}
                        }
                    }
                }
            }
        }
    
    def get_resource_from_url(self, url: str) -> Dict[str, Any]:
        """ä»URLè·å–èµ„æº
        
        Args:
            url: èµ„æºURL
            
        Returns:
            èµ„æºå†…å®¹å­—å…¸
        """
        try:
            response = requests.get(url, timeout=self.config["timeout"])
            response.raise_for_status()
            return {
                "status": "success",
                "data": response.json(),
                "message": f"Successfully fetched resource from {url}"
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "message": f"Failed to fetch resource from {url}"
            }
    
    def detect_workflow_errors(self, workflow: Dict[str, Any]) -> Dict[str, Any]:
        """æ£€æµ‹å·¥ä½œæµé”™è¯¯
        
        Args:
            workflow: å·¥ä½œæµæ•°æ®
            
        Returns:
            é”™è¯¯æ£€æµ‹ç»“æœ
        """
        errors = []
        warnings = []
        
        # æ£€æŸ¥å¿…å¡«å­—æ®µ
        required_fields = self.workflow_schema.get("required", [])
        for field in required_fields:
            if field not in workflow:
                errors.append({
                    "type": "missing_required_field",
                    "field": field,
                    "message": f"Missing required field: {field}"
                })
        
        # æ£€æŸ¥èŠ‚ç‚¹
        if "nodes" in workflow:
            nodes = workflow["nodes"]
            node_ids = set()
            
            for i, node in enumerate(nodes):
                # æ£€æŸ¥èŠ‚ç‚¹å¿…å¡«å­—æ®µ
                node_required = self.workflow_schema["properties"]["nodes"]["items"]["required"]
                for field in node_required:
                    if field not in node:
                        errors.append({
                            "type": "missing_node_field",
                            "node_index": i,
                            "field": field,
                            "message": f"Node {i} missing required field: {field}"
                        })
                
                # æ£€æŸ¥èŠ‚ç‚¹IDå”¯ä¸€æ€§
                if "id" in node:
                    if node["id"] in node_ids:
                        errors.append({
                            "type": "duplicate_node_id",
                            "node_id": node["id"],
                            "message": f"Duplicate node ID: {node['id']}"
                        })
                    node_ids.add(node["id"])
        
        # æ£€æŸ¥è¾¹
        if "edges" in workflow:
            edges = workflow["edges"]
            edge_ids = set()
            
            for i, edge in enumerate(edges):
                # æ£€æŸ¥è¾¹å¿…å¡«å­—æ®µ
                edge_required = self.workflow_schema["properties"]["edges"]["items"]["required"]
                for field in edge_required:
                    if field not in edge:
                        errors.append({
                            "type": "missing_edge_field",
                            "edge_index": i,
                            "field": field,
                            "message": f"Edge {i} missing required field: {field}"
                        })
                
                # æ£€æŸ¥è¾¹IDå”¯ä¸€æ€§
                if "id" in edge:
                    if edge["id"] in edge_ids:
                        errors.append({
                            "type": "duplicate_edge_id",
                            "edge_id": edge["id"],
                            "message": f"Duplicate edge ID: {edge['id']}"
                        })
                    edge_ids.add(edge["id"])
                
                # æ£€æŸ¥è¾¹å¼•ç”¨çš„èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨
                if "source" in edge and "target" in edge and "nodes" in workflow:
                    node_ids = {node["id"] for node in workflow["nodes"]}
                    if edge["source"] not in node_ids:
                        errors.append({
                            "type": "invalid_edge_source",
                            "edge_id": edge.get("id", f"edge_{i}"),
                            "source": edge["source"],
                            "message": f"Edge references non-existent source node: {edge['source']}"
                        })
                    if edge["target"] not in node_ids:
                        errors.append({
                            "type": "invalid_edge_target",
                            "edge_id": edge.get("id", f"edge_{i}"),
                            "target": edge["target"],
                            "message": f"Edge references non-existent target node: {edge['target']}"
                        })
        
        return {
            "status": "success",
            "errors": errors,
            "warnings": warnings,
            "total_errors": len(errors),
            "total_warnings": len(warnings)
        }
    
    def fix_workflow_errors(self, workflow: Dict[str, Any], error_report: Dict[str, Any]) -> Dict[str, Any]:
        """ä¿®å¤å·¥ä½œæµé”™è¯¯
        
        Args:
            workflow: å·¥ä½œæµæ•°æ®
            error_report: é”™è¯¯æ£€æµ‹æŠ¥å‘Š
            
        Returns:
            ä¿®å¤åçš„å·¥ä½œæµå’Œä¿®å¤æŠ¥å‘Š
        """
        fixed_workflow = workflow.copy()
        fixes = []
        
        # ä¿®å¤ç¼ºå¤±çš„å¿…å¡«å­—æ®µ
        required_fields = self.workflow_schema.get("required", [])
        for field in required_fields:
            if field not in fixed_workflow:
                if field == "name":
                    fixed_workflow["name"] = "Untitled Workflow"
                    fixes.append({
                        "type": "added_required_field",
                        "field": field,
                        "value": fixed_workflow[field],
                        "message": f"Added missing required field '{field}' with default value"
                    })
                elif field == "description":
                    fixed_workflow["description"] = "Automatically generated workflow"
                    fixes.append({
                        "type": "added_required_field",
                        "field": field,
                        "value": fixed_workflow[field],
                        "message": f"Added missing required field '{field}' with default value"
                    })
                elif field == "nodes":
                    fixed_workflow["nodes"] = []
                    fixes.append({
                        "type": "added_required_field",
                        "field": field,
                        "value": fixed_workflow[field],
                        "message": f"Added missing required field '{field}' with empty array"
                    })
                elif field == "edges":
                    fixed_workflow["edges"] = []
                    fixes.append({
                        "type": "added_required_field",
                        "field": field,
                        "value": fixed_workflow[field],
                        "message": f"Added missing required field '{field}' with empty array"
                    })
        
        # ä¿®å¤èŠ‚ç‚¹é—®é¢˜
        if "nodes" in fixed_workflow:
            nodes = fixed_workflow["nodes"]
            node_ids = set()
            
            for i, node in enumerate(nodes):
                # ç¡®ä¿èŠ‚ç‚¹IDå”¯ä¸€
                if "id" in node:
                    original_id = node["id"]
                    counter = 1
                    while node["id"] in node_ids:
                        node["id"] = f"{original_id}_{counter}"
                        counter += 1
                    if node["id"] != original_id:
                        fixes.append({
                            "type": "fixed_duplicate_node_id",
                            "node_index": i,
                            "old_id": original_id,
                            "new_id": node["id"],
                            "message": f"Fixed duplicate node ID: {original_id} -> {node['id']}"
                        })
                    node_ids.add(node["id"])
                else:
                    # æ·»åŠ ç¼ºå¤±çš„èŠ‚ç‚¹ID
                    new_id = f"node_{i}"
                    node["id"] = new_id
                    node_ids.add(new_id)
                    fixes.append({
                        "type": "added_node_id",
                        "node_index": i,
                        "new_id": new_id,
                        "message": f"Added missing node ID: {new_id}"
                    })
        
        # ä¿®å¤è¾¹é—®é¢˜
        if "edges" in fixed_workflow and "nodes" in fixed_workflow:
            edges = fixed_workflow["edges"]
            edge_ids = set()
            node_ids = {node["id"] for node in fixed_workflow["nodes"]}
            
            for i, edge in enumerate(edges):
                # ç¡®ä¿è¾¹IDå”¯ä¸€
                if "id" in edge:
                    original_id = edge["id"]
                    counter = 1
                    while edge["id"] in edge_ids:
                        edge["id"] = f"{original_id}_{counter}"
                        counter += 1
                    if edge["id"] != original_id:
                        fixes.append({
                            "type": "fixed_duplicate_edge_id",
                            "edge_index": i,
                            "old_id": original_id,
                            "new_id": edge["id"],
                            "message": f"Fixed duplicate edge ID: {original_id} -> {edge['id']}"
                        })
                    edge_ids.add(edge["id"])
                else:
                    # æ·»åŠ ç¼ºå¤±çš„è¾¹ID
                    new_id = f"edge_{i}"
                    edge["id"] = new_id
                    edge_ids.add(new_id)
                    fixes.append({
                        "type": "added_edge_id",
                        "edge_index": i,
                        "new_id": new_id,
                        "message": f"Added missing edge ID: {new_id}"
                    })
        
        return {
            "status": "success",
            "fixed_workflow": fixed_workflow,
            "fixes": fixes,
            "total_fixes": len(fixes)
        }
    
    def analyze_code_quality(self, code: str, language: str = "python") -> Dict[str, Any]:
        """åˆ†æä»£ç è´¨é‡
        
        Args:
            code: ä»£ç å†…å®¹
            language: ä»£ç è¯­è¨€
            
        Returns:
            ä»£ç è´¨é‡åˆ†ææŠ¥å‘Š
        """
        # ç®€åŒ–çš„ä»£ç è´¨é‡åˆ†æï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥é›†æˆæ›´å¤æ‚çš„åˆ†æå·¥å…·
        issues = []
        
        # æ£€æŸ¥ä»£ç è¡Œæ•°
        lines = code.split('\n')
        total_lines = len(lines)
        
        # æ£€æŸ¥ç©ºè¡Œ
        empty_lines = sum(1 for line in lines if line.strip() == '')
        
        # æ£€æŸ¥æ³¨é‡Šæ¯”ä¾‹
        comment_lines = sum(1 for line in lines if line.strip().startswith('#') or '#' in line)
        comment_ratio = comment_lines / total_lines if total_lines > 0 else 0
        
        # æ£€æŸ¥é•¿è¡Œ
        long_lines = [i+1 for i, line in enumerate(lines) if len(line) > 80]
        for line_num in long_lines:
            issues.append({
                "type": "long_line",
                "line": line_num,
                "message": f"Line {line_num} is too long (exceeds 80 characters)"
            })
        
        # æ£€æŸ¥é‡å¤ä»£ç ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        line_counts = {}
        for line in lines:
            stripped = line.strip()
            if stripped and not stripped.startswith('#'):
                line_counts[stripped] = line_counts.get(stripped, 0) + 1
        
        for line, count in line_counts.items():
            if count > 5:
                issues.append({
                    "type": "repeated_code",
                    "line_content": line,
                    "count": count,
                    "message": f"Code line repeated {count} times: {line}"
                })
        
        # æ£€æŸ¥ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²
        if language == "python":
            # ç®€å•æ£€æŸ¥å‡½æ•°ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²
            function_pattern = r'def\s+\w+\s*\([^)]*\)\s*:'
            functions = re.finditer(function_pattern, code)
            
            for match in functions:
                function_start = match.end()
                # æ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æ˜¯æ–‡æ¡£å­—ç¬¦ä¸²
                next_line = code[function_start:].split('\n')[0].strip()
                if not (next_line.startswith('"""') or next_line.startswith("'""")):
                    issues.append({
                        "type": "missing_docstring",
                        "function": match.group(),
                        "message": f"Function {match.group()} missing docstring"
                    })
        
        return {
            "status": "success",
            "language": language,
            "total_lines": total_lines,
            "empty_lines": empty_lines,
            "comment_lines": comment_lines,
            "comment_ratio": round(comment_ratio, 2),
            "issues": issues,
            "total_issues": len(issues)
        }
    
    def generate_workflow(self, requirements: str) -> Dict[str, Any]:
        """æ ¹æ®éœ€æ±‚ç”Ÿæˆå·¥ä½œæµ
        
        Args:
            requirements: å·¥ä½œæµéœ€æ±‚æè¿°
            
        Returns:
            ç”Ÿæˆçš„å·¥ä½œæµ
        """
        # ç®€åŒ–çš„å·¥ä½œæµç”Ÿæˆï¼Œå®é™…é¡¹ç›®ä¸­å¯ä»¥é›†æˆLLMæ¥ç”Ÿæˆæ›´å¤æ‚çš„å·¥ä½œæµ
        workflow = {
            "name": "Generated Workflow",
            "description": f"Automatically generated from requirements: {requirements}",
            "version": "1.0.0",
            "nodes": [
                {
                    "id": "start",
                    "type": "start",
                    "data": {
                        "label": "Start",
                        "description": "Workflow start node"
                    },
                    "position": {"x": 100, "y": 100}
                },
                {
                    "id": "process",
                    "type": "process",
                    "data": {
                        "label": "Process",
                        "description": "Main processing node",
                        "requirements": requirements
                    },
                    "position": {"x": 300, "y": 100}
                },
                {
                    "id": "end",
                    "type": "end",
                    "data": {
                        "label": "End",
                        "description": "Workflow end node"
                    },
                    "position": {"x": 500, "y": 100}
                }
            ],
            "edges": [
                {
                    "id": "edge_start_process",
                    "source": "start",
                    "target": "process"
                },
                {
                    "id": "edge_process_end",
                    "source": "process",
                    "target": "end"
                }
            ]
        }
        
        return {
            "status": "success",
            "workflow": workflow,
            "requirements": requirements
        }
    
    def auto_fill_canvas(self, workflow: Dict[str, Any]) -> Dict[str, Any]:
        """è‡ªåŠ¨åŒ–å¡«å†™Cozeç”»å¸ƒ
        
        Args:
            workflow: å·¥ä½œæµæ•°æ®
            
        Returns:
            å¡«å†™å®Œæˆçš„ç”»å¸ƒæ•°æ®
        """
        # ç®€åŒ–çš„ç”»å¸ƒå¡«å†™ï¼Œå®é™…é¡¹ç›®ä¸­éœ€è¦æ ¹æ®Cozeç”»å¸ƒçš„å…·ä½“æ ¼å¼è¿›è¡Œå¡«å†™
        canvas_data = {
            "workflow": workflow,
            "canvas_settings": {
                "title": workflow.get("name", "Untitled Canvas"),
                "description": workflow.get("description", ""),
                "version": workflow.get("version", "1.0.0"),
                "theme": "default",
                "zoom": 1.0,
                "centerX": 0,
                "centerY": 0
            },
            "nodes_positioned": True,
            "auto_layout": True
        }
        
        # è‡ªåŠ¨å¸ƒå±€èŠ‚ç‚¹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        if "nodes" in workflow:
            nodes = workflow["nodes"]
            for i, node in enumerate(nodes):
                if "position" not in node:
                    node["position"] = {
                        "x": 100 + (i * 200),
                        "y": 100
                    }
        
        return {
            "status": "success",
            "canvas_data": canvas_data,
            "message": "Canvas automatically filled"
        }
    
    def batch_process_workflows(self, workflows: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ‰¹é‡å¤„ç†å·¥ä½œæµ
        
        Args:
            workflows: å·¥ä½œæµåˆ—è¡¨
            
        Returns:
            æ‰¹é‡å¤„ç†ç»“æœ
        """
        results = []
        
        for i, workflow in enumerate(workflows):
            # æ£€æµ‹é”™è¯¯
            error_report = self.detect_workflow_errors(workflow)
            
            # ä¿®å¤é”™è¯¯
            fix_result = self.fix_workflow_errors(workflow, error_report)
            
            # å¡«å†™ç”»å¸ƒ
            canvas_result = self.auto_fill_canvas(fix_result["fixed_workflow"])
            
            results.append({
                "original_index": i,
                "error_detection": error_report,
                "fix_result": fix_result,
                "canvas_result": canvas_result
            })
        
        return {
            "status": "success",
            "results": results,
            "total_workflows": len(workflows),
            "processed_workflows": len(results)
        }
    
    def merge_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ™ºèƒ½åˆå¹¶ç»“æœ
        
        Args:
            results: ç»“æœåˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„ç»“æœ
        """
        merged = {
            "status": "success",
            "total_results": len(results),
            "merged_at": time.time(),
            "summary": {
                "success_count": 0,
                "error_count": 0,
                "warning_count": 0
            },
            "detailed_results": []
        }
        
        for result in results:
            if result["status"] == "success":
                merged["summary"]["success_count"] += 1
            else:
                merged["summary"]["error_count"] += 1
            
            merged["detailed_results"].append(result)
        
        return merged
    
    def process(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸»å¤„ç†å‡½æ•°
        
        Args:
            input_data: è¾“å…¥æ•°æ®ï¼ŒåŒ…å«å¤„ç†ç±»å‹å’Œå‚æ•°
            
        Returns:
            å¤„ç†ç»“æœ
        """
        process_type = input_data.get("type", "")
        
        if process_type == "fetch_resource":
            # ä»URLè·å–èµ„æº
            url = input_data.get("url", "")
            if not url:
                return {"status": "error", "message": "Missing URL parameter"}
            return self.get_resource_from_url(url)
        
        elif process_type == "detect_errors":
            # æ£€æµ‹å·¥ä½œæµé”™è¯¯
            workflow = input_data.get("workflow", {})
            return self.detect_workflow_errors(workflow)
        
        elif process_type == "fix_errors":
            # ä¿®å¤å·¥ä½œæµé”™è¯¯
            workflow = input_data.get("workflow", {})
            error_report = input_data.get("error_report", {})
            if not error_report:
                error_report = self.detect_workflow_errors(workflow)
            return self.fix_workflow_errors(workflow, error_report)
        
        elif process_type == "analyze_code":
            # åˆ†æä»£ç è´¨é‡
            code = input_data.get("code", "")
            language = input_data.get("language", "python")
            if not code:
                return {"status": "error", "message": "Missing code parameter"}
            return self.analyze_code_quality(code, language)
        
        elif process_type == "generate_workflow":
            # ç”Ÿæˆå·¥ä½œæµ
            requirements = input_data.get("requirements", "")
            if not requirements:
                return {"status": "error", "message": "Missing requirements parameter"}
            return self.generate_workflow(requirements)
        
        elif process_type == "auto_fill_canvas":
            # è‡ªåŠ¨å¡«å†™ç”»å¸ƒ
            workflow = input_data.get("workflow", {})
            if not workflow:
                return {"status": "error", "message": "Missing workflow parameter"}
            return self.auto_fill_canvas(workflow)
        
        elif process_type == "batch_process":
            # æ‰¹é‡å¤„ç†
            workflows = input_data.get("workflows", [])
            if not workflows:
                return {"status": "error", "message": "Missing workflows parameter"}
            return self.batch_process_workflows(workflows)
        
        elif process_type == "merge_results":
            # åˆå¹¶ç»“æœ
            results = input_data.get("results", [])
            if not results:
                return {"status": "error", "message": "Missing results parameter"}
            return self.merge_results(results)
        
        else:
            return {
                "status": "error",
                "message": f"Unknown process type: {process_type}",
                "supported_types": [
                    "fetch_resource", "detect_errors", "fix_errors", 
                    "analyze_code", "generate_workflow", "auto_fill_canvas",
                    "batch_process", "merge_results"
                ]
            }
    
    def run_as_server(self, host: str = "localhost", port: int = 8080):
        """ä»¥HTTPæœåŠ¡å™¨æ¨¡å¼è¿è¡Œ
        
        Args:
            host: æœåŠ¡å™¨ä¸»æœº
            port: æœåŠ¡å™¨ç«¯å£
        """
        from flask import Flask, request, jsonify
        
        app = Flask(__name__)
        
        @app.route('/process', methods=['POST'])
        def api_process():
            try:
                input_data = request.json
                result = self.process(input_data)
                return jsonify(result)
            except Exception as e:
                return jsonify({
                    "status": "error",
                    "message": str(e)
                }), 500
        
        @app.route('/health', methods=['GET'])
        def health_check():
            return jsonify({
                "status": "healthy",
                "service": "AI_Automated_Workflow_Processo",
                "timestamp": time.time()
            })
        
        print(f"ğŸš€ AI_Automated_Workflow_Processo æœåŠ¡å™¨å¯åŠ¨")
        print(f"ğŸ“¡ ç›‘å¬åœ°å€: http://{host}:{port}")
        print(f"ğŸ’¡ å¥åº·æ£€æŸ¥: http://{host}:{port}/health")
        print(f"ğŸ”§ APIç«¯ç‚¹: http://{host}:{port}/process")
        print(f"ğŸ“š æ”¯æŒçš„å¤„ç†ç±»å‹: fetch_resource, detect_errors, fix_errors, analyze_code, generate_workflow, auto_fill_canvas, batch_process, merge_results")
        print(f"\næŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨")
        
        app.run(host=host, port=port)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='AI_Automated_Workflow_Processo - å®Œæ•´çš„AIè‡ªåŠ¨åŒ–å·¥ä½œæµå¤„ç†å™¨')
    parser.add_argument('--server', action='store_true', help='ä»¥HTTPæœåŠ¡å™¨æ¨¡å¼è¿è¡Œ')
    parser.add_argument('--host', type=str, default='localhost', help='æœåŠ¡å™¨ä¸»æœºåœ°å€')
    parser.add_argument('--port', type=int, default=8080, help='æœåŠ¡å™¨ç«¯å£')
    parser.add_argument('--test', action='store_true', help='è¿è¡Œæµ‹è¯•')
    
    args = parser.parse_args()
    
    processor = AIWorkflowProcessor()
    
    if args.test:
        # è¿è¡Œæµ‹è¯•
        print("ğŸ” è¿è¡ŒAIè‡ªåŠ¨åŒ–å·¥ä½œæµå¤„ç†å™¨æµ‹è¯•...")
        
        # æµ‹è¯•1: ç”Ÿæˆå·¥ä½œæµ
        print("\n1. æµ‹è¯•å·¥ä½œæµç”ŸæˆåŠŸèƒ½...")
        generate_result = processor.generate_workflow("åˆ›å»ºä¸€ä¸ªç®€å•çš„ç”¨æˆ·æ³¨å†Œå·¥ä½œæµ")
        print(f"   ç»“æœ: {generate_result['status']}")
        print(f"   ç”Ÿæˆçš„å·¥ä½œæµåç§°: {generate_result['workflow']['name']}")
        print(f"   èŠ‚ç‚¹æ•°é‡: {len(generate_result['workflow']['nodes'])}")
        print(f"   è¾¹æ•°é‡: {len(generate_result['workflow']['edges'])}")
        
        # æµ‹è¯•2: æ£€æµ‹å·¥ä½œæµé”™è¯¯
        print("\n2. æµ‹è¯•é”™è¯¯æ£€æµ‹åŠŸèƒ½...")
        # åˆ›å»ºä¸€ä¸ªæœ‰é”™è¯¯çš„å·¥ä½œæµ
        broken_workflow = {
            "name": "Broken Workflow",
            "nodes": [
                {"type": "start", "data": {"label": "Start"}}  # ç¼ºå°‘idå­—æ®µ
            ]
        }
        error_result = processor.detect_workflow_errors(broken_workflow)
        print(f"   ç»“æœ: {error_result['status']}")
        print(f"   é”™è¯¯æ•°é‡: {error_result['total_errors']}")
        
        # æµ‹è¯•3: ä¿®å¤å·¥ä½œæµé”™è¯¯
        print("\n3. æµ‹è¯•å·¥ä½œæµä¿®å¤åŠŸèƒ½...")
        fix_result = processor.fix_workflow_errors(broken_workflow, error_result)
        print(f"   ç»“æœ: {fix_result['status']}")
        print(f"   ä¿®å¤æ•°é‡: {fix_result['total_fixes']}")
        print(f"   ä¿®å¤åæ˜¯å¦åŒ…å«edgeså­—æ®µ: {'edges' in fix_result['fixed_workflow']}")
        
        # æµ‹è¯•4: ä»£ç è´¨é‡åˆ†æ
        print("\n4. æµ‹è¯•ä»£ç è´¨é‡åˆ†æåŠŸèƒ½...")
        test_code = '''
def add(a, b):
    return a + b

print(add(1, 2))
'''        
        code_analysis = processor.analyze_code_quality(test_code)
        print(f"   ç»“æœ: {code_analysis['status']}")
        print(f"   ä»£ç è¡Œæ•°: {code_analysis['total_lines']}")
        print(f"   æ³¨é‡Šæ¯”ä¾‹: {code_analysis['comment_ratio']}")
        print(f"   é—®é¢˜æ•°é‡: {code_analysis['total_issues']}")
        
        # æµ‹è¯•5: è‡ªåŠ¨å¡«å†™ç”»å¸ƒ
        print("\n5. æµ‹è¯•ç”»å¸ƒè‡ªåŠ¨åŒ–å¡«å†™åŠŸèƒ½...")
        canvas_result = processor.auto_fill_canvas(generate_result['workflow'])
        print(f"   ç»“æœ: {canvas_result['status']}")
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        
    elif args.server:
        # å¯åŠ¨æœåŠ¡å™¨
        processor.run_as_server(args.host, args.port)
    else:
        # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
        parser.print_help()


if __name__ == '__main__':
    main()
